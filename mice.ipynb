{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jo9w80Bhx70m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hongh-zhang/Vision.git\n",
        "path = 'Vision/data/mice/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZozSvSpWjfmv",
        "outputId": "19ad6de8-4409-413f-b291-6c8f7c700321"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CV-playground'...\n",
            "remote: Enumerating objects: 273, done.\u001b[K\n",
            "remote: Counting objects: 100% (273/273), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 273 (delta 7), reused 267 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (273/273), 35.05 MiB | 19.22 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocess"
      ],
      "metadata": {
        "id": "ZV5TQufWuMVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading & preprocessing labels\n",
        "# (x,y) coordinates of each body part are provided in the label csv\n",
        "# so we'll have to convert it into array/tensors\n",
        "# each label coordinate & surrounding pixels are represented as 1\n",
        "# the rest being 0 to make up a heatmap\n",
        "\n",
        "PXL_RANGE = (-1.9999, -0.9999, 0.0001, 1.0001, 2.0001)\n",
        "# how many pixels away from the label is treated as 1.0\n",
        "# (3 here)\n",
        "# python's rounding sometimes ties on 0.5 e.g. both 13.5, 14.5 -> 14\n",
        "# so I added 1e-4\n",
        "PXLS = [1.0 for _ in range(len(PXL_RANGE)**2)]\n",
        "\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for img_file, row in zip(sorted(os.listdir(path+'frames')), \n",
        "            pd.read_csv(path+'CollectedData_Pranav.csv').iloc[2:,1:].to_numpy(dtype=float)):\n",
        "    \n",
        "    # read image into (1, 480, 640, 3) tesnor\n",
        "    img = cv2.imread(os.path.join(path+'frames',img_file))\n",
        "    img = tf.convert_to_tensor(img.reshape(1, 480, 640, 3), dtype=float)/255.0\n",
        "    \n",
        "    # process label coordinate into heatmap\n",
        "    lbl = []\n",
        "    for i in range(0, 8, 2):\n",
        "        x = row[i+1] / 2  # downsample by 2\n",
        "        y = row[i] / 2\n",
        "        indices = sorted([[round(x+j), round(y+k)] \n",
        "                  for j in PXL_RANGE for k in PXL_RANGE\n",
        "                  if round(x+j)>=0 and round(y+k)>=0])\n",
        "        \n",
        "        # calling sparse tensor to easily input indices\n",
        "        heatmap = tf.sparse.SparseTensor(indices, PXLS, (240, 320))\n",
        "        heatmap = tf.reshape(tf.sparse.to_dense(heatmap), (1, 240, 320, 1))\n",
        "        lbl.append(heatmap)\n",
        "    lbl = tf.concat(lbl, axis=3)  # each lbl: (1, 480, 640, 4)\n",
        "    \n",
        "    images.append(img)\n",
        "    labels.append(lbl)\n",
        "\n",
        "    # flip\n",
        "    for axis in ([1],[2],[1,2]):\n",
        "        img = tf.reverse(img, axis)\n",
        "        lbl = tf.reverse(lbl, axis)\n",
        "        images.append(img)\n",
        "        labels.append(lbl)\n",
        "\n",
        "# convert list of tensors into a large tensor\n",
        "images = tf.concat(images, axis=0)\n",
        "labels = tf.concat(labels, axis=0)\n",
        "\n",
        "# cast labels into 1+4d, -> (None, 240, 320, 4, 2)\n",
        "labels = tf.stack([1-labels, labels], axis=4)\n",
        "print(labels.shape)\n",
        "# each dim corresponds to\n",
        "# (batch size, height, width,\n",
        "# no. of feature, we have (snout, L.ear, R.ear, tail) here\n",
        "# , probability of feature/non-feature)\n",
        "\n",
        "# the last dimension is actually redundant, but required to work with\n",
        "# tensorflow's categorical cross entropy loss\n",
        "# (didn't manage to properly use the binary cross entropy loss, maybe\n",
        "# a custom loss is required?)\n",
        "\n",
        "cv2_imshow(labels[0].numpy()[:,:,0,1] * 255)\n",
        "# -> snout position of the above image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "qnN9t-Btna88",
        "outputId": "5f45c7c7-89f9-48b1-d5aa-95ffd9f3edb2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(464, 240, 320, 4, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAADwCAAAAABURuK3AAAAeklEQVR4nO3QMQoAIAwEwej//6ydYBdIIcGZKumOjQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAr41zrfslZ74e0J2ARQIWCQgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAsbjT4BCtLCu6QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=320x240 at 0x7F3411CD1B10>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "rE0bE2bnmQGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self,):\n",
        "    super().__init__(self)\n",
        "    self.resNet = tf.keras.applications.resnet50.ResNet50(include_top=False, \n",
        "            weights='imagenet', input_shape=(480, 640, 3), pooling=None)\n",
        "\n",
        "    self.up = tf.keras.Sequential([\n",
        "        layers.UpSampling2D(size=(2,2)),\n",
        "        layers.Conv2D(1024, 3, padding='same', activation='ReLU'),\n",
        "        layers.UpSampling2D(size=(2,2)),\n",
        "        layers.Conv2D(512, 3, padding='same', activation='ReLU'),\n",
        "        layers.UpSampling2D(size=(2,2)),\n",
        "        layers.Conv2D(256, 3, padding='same', activation='ReLU'),\n",
        "        layers.UpSampling2D(size=(2,2)),\n",
        "        layers.Conv2D(128, 3, padding='same', activation='ReLU')\n",
        "    ])\n",
        "\n",
        "    self.out = tf.keras.Sequential([\n",
        "        layers.Conv2D(64, 3, padding='same', activation='ReLU'),\n",
        "        layers.Conv2D(4, 1, padding='same', activation=None)\n",
        "    ])\n",
        "    self.activate = layers.Activation('softmax')\n",
        "\n",
        "  def call(self, X, training=False):\n",
        "      X0 = tf.nn.max_pool(X, ksize=2, strides=2, padding=\"VALID\")\n",
        "      X = self.resNet(X)\n",
        "      X = self.up(X)\n",
        "      X = tf.concat([X0, X], axis=3)\n",
        "      X = self.out(X)\n",
        "\n",
        "      \n",
        "      X = tf.stack([1-X, X], axis=4)\n",
        "      X = self.activate(X)\n",
        "      return X"
      ],
      "metadata": {
        "id": "zr8lBcugyN1Q"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Model()\n",
        "print(nn(np.zeros((1, 480, 640, 3))).shape)\n",
        "nn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-thjQb8FyN3-",
        "outputId": "3d190d73-91b2-4ce0-eca6-85023118e61d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 240, 320, 4, 2)\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 15, 20, 2048)      23587712  \n",
            "                                                                 \n",
            " sequential_13 (Sequential)  (1, 240, 320, 128)        25069440  \n",
            "                                                                 \n",
            " sequential_14 (Sequential)  (1, 240, 320, 4)          75780     \n",
            "                                                                 \n",
            " activation_2 (Activation)   multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48,732,932\n",
            "Trainable params: 48,679,812\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4, name='Adam'), \n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "    )\n",
        "nn.fit(x=images, y=labels, shuffle=True, batch_size=4, epochs=20, verbose='auto');\n",
        "# larger batch size breaks colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdm5F05W2iej",
        "outputId": "7151ac49-88c1-4246-cb2a-6d5fb891b152"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "116/116 [==============================] - 272s 2s/step - loss: 0.0347\n",
            "Epoch 2/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 0.0014\n",
            "Epoch 3/20\n",
            "116/116 [==============================] - 266s 2s/step - loss: 0.0012\n",
            "Epoch 4/20\n",
            "116/116 [==============================] - 266s 2s/step - loss: 0.0011\n",
            "Epoch 5/20\n",
            "116/116 [==============================] - 266s 2s/step - loss: 0.0010\n",
            "Epoch 6/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 8.9876e-04\n",
            "Epoch 7/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 7.8567e-04\n",
            "Epoch 8/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 7.1083e-04\n",
            "Epoch 9/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 6.5005e-04\n",
            "Epoch 10/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 5.7482e-04\n",
            "Epoch 11/20\n",
            "116/116 [==============================] - 264s 2s/step - loss: 5.2116e-04\n",
            "Epoch 12/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 4.8487e-04\n",
            "Epoch 13/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 4.5482e-04\n",
            "Epoch 14/20\n",
            "116/116 [==============================] - 264s 2s/step - loss: 4.1378e-04\n",
            "Epoch 15/20\n",
            "116/116 [==============================] - 264s 2s/step - loss: 4.0432e-04\n",
            "Epoch 16/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 3.8281e-04\n",
            "Epoch 17/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 3.8033e-04\n",
            "Epoch 18/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 3.7001e-04\n",
            "Epoch 19/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 3.3353e-04\n",
            "Epoch 20/20\n",
            "116/116 [==============================] - 265s 2s/step - loss: 3.1847e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "nn.save_weights('./checkpoints/my_checkpoint')\n",
        "!zip -r checkpoint.zip checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-nVZXLr8nFr",
        "outputId": "07e960a4-8df1-4e9b-ef96-d96b879c94f6"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: checkpoints/ (stored 0%)\n",
            "  adding: checkpoints/my_checkpoint.data-00000-of-00001 (deflated 9%)\n",
            "  adding: checkpoints/checkpoint (deflated 49%)\n",
            "  adding: checkpoints/my_checkpoint.index (deflated 81%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(nn(images[0:1])[0,:,:,3,1].numpy() * 255)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "IQOSFtElcTKN",
        "outputId": "7c18328d-1c04-4c82-a503-40c300748597"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAADwCAAAAABURuK3AAAA3UlEQVR4nO3WMQ4BQRSA4d1laTQSPcdwA6dQO5JWo5A4gMoZ6OgodRTLhhUOMZtMNvt9/Uv+vORlJkkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABosDRsvNvJPlU9JQ0VtsDBbDw67h41tbRPPj9cb+dFJ3ZHYw239/L93PRid8SUhQx/iipJ09+vrpgmClpgsbq8qurU7lckSD5d7teT2BVRBX5j0m7/W7b6hAEAAAAAAAAAAAAAAAAAAAAAoFX+GZAgBstq8IIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=320x240 at 0x7F3417EB1790>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf outputs\n",
        "!mkdir outputs\n",
        "for i in range(len(images)):\n",
        "    for part in (0,1,2,3):\n",
        "        y = nn(images[i:i+1])[0,:,:,part,1].numpy() * 255\n",
        "        cv2.imwrite(f'outputs/{i}_{part}.jpg', y)\n",
        "!zip -r outputs.zip outputs"
      ],
      "metadata": {
        "id": "D00iM_18W6NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_A6ENwh9VjMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}