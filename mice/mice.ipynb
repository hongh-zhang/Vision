{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jo9w80Bhx70m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hongh-zhang/Vision.git\n",
        "path = 'Vision/mice/data/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZozSvSpWjfmv",
        "outputId": "5de27090-f5bb-463a-b99e-f1f905c2e636"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Vision' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocess"
      ],
      "metadata": {
        "id": "ZV5TQufWuMVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading & preprocessing labels\n",
        "# (x,y) coordinates of each body part are provided in the label csv\n",
        "# so we'll have to convert it into array/tensors\n",
        "# each label coordinate & surrounding pixels are represented as 1\n",
        "# the rest being 0 to make up a heatmap\n",
        "\n",
        "PXL_RANGE = (-1.9999, -0.9999, 0.0001, 1.0001, 2.0001)\n",
        "# how many pixels away from the label is treated as 1.0\n",
        "# (3 here)\n",
        "# python's rounding sometimes ties on 0.5 e.g. both 13.5, 14.5 -> 14\n",
        "# so I added 1e-4\n",
        "PXLS = [1.0 for _ in range(len(PXL_RANGE)**2)]\n",
        "\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for img_file, row in zip(sorted(os.listdir(path+'frames')), \n",
        "            pd.read_csv(path+'CollectedData_Pranav.csv').iloc[2:,1:].to_numpy(dtype=float)):\n",
        "    \n",
        "    # read image into (1, 480, 640, 3) tesnor\n",
        "    img = cv2.imread(os.path.join(path+'frames',img_file))\n",
        "    img = tf.convert_to_tensor(img.reshape(1, 480, 640, 3), dtype=float)/255.0\n",
        "    \n",
        "    # process label coordinate into heatmap\n",
        "    lbl = []\n",
        "    for i in range(0, 8, 2):\n",
        "        x = row[i+1] / 2  # downsample by 2\n",
        "        y = row[i] / 2\n",
        "        indices = sorted([[round(x+j), round(y+k)] \n",
        "                  for j in PXL_RANGE for k in PXL_RANGE\n",
        "                  if round(x+j)>=0 and round(y+k)>=0])\n",
        "        \n",
        "        # calling sparse tensor to easily input indices\n",
        "        heatmap = tf.sparse.SparseTensor(indices, PXLS, (240, 320))\n",
        "        heatmap = tf.reshape(tf.sparse.to_dense(heatmap), (1, 240, 320, 1))\n",
        "        lbl.append(heatmap)\n",
        "    lbl = tf.concat(lbl, axis=3)  # each lbl: (1, 480, 640, 4)\n",
        "    \n",
        "    images.append(img)\n",
        "    labels.append(lbl)\n",
        "\n",
        "    # flip\n",
        "    for axis in ([1],[2],[1,2]):\n",
        "        new_img = tf.reverse(img, axis)\n",
        "        new_lbl = tf.reverse(lbl, axis)\n",
        "        images.append(new_img)\n",
        "        labels.append(new_lbl)\n",
        "\n",
        "# convert list of tensors into a large tensor\n",
        "images = tf.concat(images, axis=0)\n",
        "labels = tf.concat(labels, axis=0)\n",
        "\n",
        "# cast labels into 1+4d, -> (None, 240, 320, 4, 2)\n",
        "labels = tf.stack([1-labels, labels], axis=4)\n",
        "print(labels.shape)\n",
        "# each dim corresponds to\n",
        "# (batch size, height, width,\n",
        "# no. of feature, we have (snout, L.ear, R.ear, tail) here,\n",
        "# probability of feature/non-feature)\n",
        "\n",
        "# the last dimension is actually redundant, but required to work with\n",
        "# tensorflow's categorical cross entropy loss\n",
        "# (didn't manage to properly use the binary cross entropy loss, maybe\n",
        "# a custom loss is required?)\n",
        "\n",
        "cv2_imshow(labels[0].numpy()[:,:,0,1] * 255)\n",
        "# -> snout position of 1st image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "qnN9t-Btna88",
        "outputId": "3041b969-0567-49bc-e0a3-c74b36add39e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(464, 240, 320, 4, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAADwCAAAAABURuK3AAAAeklEQVR4nO3QMQoAIAwEwej//6ydYBdIIcGZKumOjQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAr41zrfslZ74e0J2ARQIWCQgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAsbjT4BCtLCu6QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=320x240 at 0x7F83ECC98B50>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = images[:20]\n",
        "test_labels = labels[:20]\n",
        "images = images[20:]\n",
        "labels = labels[20:]"
      ],
      "metadata": {
        "id": "5zDqvoUjUjan"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "rE0bE2bnmQGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "    def __init__(self,):\n",
        "        super().__init__(self)\n",
        "\n",
        "        self.dropout = layers.Dropout(0.1)\n",
        "\n",
        "        self.resNet = tf.keras.applications.resnet50.ResNet50(include_top=False, \n",
        "                weights='imagenet', input_shape=(480, 640, 3), pooling=None)\n",
        "\n",
        "        self.up = tf.keras.Sequential([\n",
        "            layers.UpSampling2D(size=(2,2)),\n",
        "            layers.Conv2D(1024, 3, padding='same', activation='ReLU'),\n",
        "            layers.UpSampling2D(size=(2,2)),\n",
        "            layers.Conv2D(512, 3, padding='same', activation='ReLU'),\n",
        "\n",
        "            layers.UpSampling2D(size=(2,2)),\n",
        "            layers.Conv2D(256, 3, padding='same', activation='ReLU'),\n",
        "            layers.UpSampling2D(size=(2,2)),\n",
        "            layers.Conv2D(128, 3, padding='same', activation=None),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('relu'),\n",
        "        ])\n",
        "\n",
        "        self.out = tf.keras.Sequential([\n",
        "            layers.Conv2D(64, 3, padding='same', activation='ReLU'),\n",
        "            layers.Conv2D(4, 1, padding='same', activation=None)\n",
        "        ])\n",
        "        self.activate = layers.Activation('softmax')\n",
        "\n",
        "    def call(self, X, training=False):\n",
        "\n",
        "        # add more augmentation\n",
        "        #X = self.dropout(X, training=training)\n",
        "        \n",
        "        # save for later concat\n",
        "        X0 = tf.nn.max_pool(X, ksize=2, strides=2, padding=\"VALID\")\n",
        "\n",
        "        # extract features with resNet, then upsample to 320*240\n",
        "        X = self.resNet(X)\n",
        "        X = self.up(X, training=training)\n",
        "\n",
        "        # concat high resolution map\n",
        "        X = tf.concat([X0, X], axis=3)\n",
        "        X = self.out(X)\n",
        "\n",
        "        # rearragne into binary format probability\n",
        "        X = tf.stack([1-X, X], axis=4)\n",
        "        X = self.activate(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "zr8lBcugyN1Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect();\n",
        "\n",
        "nn = Model()\n",
        "print(nn(np.zeros((1, 480, 640, 3))).shape)\n",
        "nn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-thjQb8FyN3-",
        "outputId": "ba1336d2-695d-46d1-f27d-e11daeec2574"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 240, 320, 4, 2)\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dropout_4 (Dropout)         multiple                  0 (unused)\n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 15, 20, 2048)      23587712  \n",
            "                                                                 \n",
            " sequential_8 (Sequential)   (1, 240, 320, 128)        25069952  \n",
            "                                                                 \n",
            " sequential_9 (Sequential)   (1, 240, 320, 4)          75780     \n",
            "                                                                 \n",
            " activation_10 (Activation)  multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48,733,444\n",
            "Trainable params: 48,680,068\n",
            "Non-trainable params: 53,376\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4, name='Adam'), \n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "    )\n",
        "nn.fit(x=images, y=labels, shuffle=True, batch_size=4, epochs=40, \n",
        "       verbose='auto', validation_data=(test_images, test_labels));\n",
        "# larger batch size breaks colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdm5F05W2iej",
        "outputId": "8eb6c346-b2cc-42c5-eb9d-b514bf3be542"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "111/111 [==============================] - 76s 639ms/step - loss: 0.0148 - val_loss: 0.0064\n",
            "Epoch 2/40\n",
            "111/111 [==============================] - 72s 653ms/step - loss: 0.0014 - val_loss: 0.0029\n",
            "Epoch 3/40\n",
            "111/111 [==============================] - 74s 667ms/step - loss: 0.0012 - val_loss: 0.0030\n",
            "Epoch 4/40\n",
            "111/111 [==============================] - 75s 675ms/step - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 5/40\n",
            "111/111 [==============================] - 76s 680ms/step - loss: 9.3094e-04 - val_loss: 0.0033\n",
            "Epoch 6/40\n",
            "111/111 [==============================] - 76s 682ms/step - loss: 8.4059e-04 - val_loss: 0.0031\n",
            "Epoch 7/40\n",
            "111/111 [==============================] - 76s 684ms/step - loss: 7.7069e-04 - val_loss: 0.0034\n",
            "Epoch 8/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 7.1889e-04 - val_loss: 0.0030\n",
            "Epoch 9/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 6.9398e-04 - val_loss: 0.0034\n",
            "Epoch 10/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 6.5529e-04 - val_loss: 0.0052\n",
            "Epoch 11/40\n",
            "111/111 [==============================] - 76s 688ms/step - loss: 6.1107e-04 - val_loss: 0.0034\n",
            "Epoch 12/40\n",
            "111/111 [==============================] - 76s 689ms/step - loss: 5.8964e-04 - val_loss: 0.0027\n",
            "Epoch 13/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 5.6558e-04 - val_loss: 0.0023\n",
            "Epoch 14/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 5.3241e-04 - val_loss: 0.0020\n",
            "Epoch 15/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 4.8040e-04 - val_loss: 9.5734e-04\n",
            "Epoch 16/40\n",
            "111/111 [==============================] - 76s 685ms/step - loss: 4.6928e-04 - val_loss: 9.1274e-04\n",
            "Epoch 17/40\n",
            "111/111 [==============================] - 76s 685ms/step - loss: 4.5590e-04 - val_loss: 8.0975e-04\n",
            "Epoch 18/40\n",
            "111/111 [==============================] - 76s 688ms/step - loss: 4.0619e-04 - val_loss: 8.4328e-04\n",
            "Epoch 19/40\n",
            "111/111 [==============================] - 76s 689ms/step - loss: 4.1048e-04 - val_loss: 8.6785e-04\n",
            "Epoch 20/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 4.0878e-04 - val_loss: 7.1664e-04\n",
            "Epoch 21/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 3.8227e-04 - val_loss: 6.3984e-04\n",
            "Epoch 22/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 3.5842e-04 - val_loss: 7.3650e-04\n",
            "Epoch 23/40\n",
            "111/111 [==============================] - 76s 688ms/step - loss: 3.5293e-04 - val_loss: 7.2734e-04\n",
            "Epoch 24/40\n",
            "111/111 [==============================] - 77s 690ms/step - loss: 3.5549e-04 - val_loss: 9.4646e-04\n",
            "Epoch 25/40\n",
            "111/111 [==============================] - 76s 688ms/step - loss: 3.5018e-04 - val_loss: 0.0013\n",
            "Epoch 26/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 3.3228e-04 - val_loss: 7.9800e-04\n",
            "Epoch 27/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 3.2131e-04 - val_loss: 7.5938e-04\n",
            "Epoch 28/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 3.1401e-04 - val_loss: 5.9885e-04\n",
            "Epoch 29/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 3.0509e-04 - val_loss: 6.2861e-04\n",
            "Epoch 30/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 3.0616e-04 - val_loss: 7.7955e-04\n",
            "Epoch 31/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 3.1064e-04 - val_loss: 7.6678e-04\n",
            "Epoch 32/40\n",
            "111/111 [==============================] - 76s 688ms/step - loss: 3.0784e-04 - val_loss: 6.8480e-04\n",
            "Epoch 33/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 2.9753e-04 - val_loss: 8.9322e-04\n",
            "Epoch 34/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 3.0268e-04 - val_loss: 6.7164e-04\n",
            "Epoch 35/40\n",
            "111/111 [==============================] - 76s 687ms/step - loss: 2.9196e-04 - val_loss: 0.0019\n",
            "Epoch 36/40\n",
            "111/111 [==============================] - 76s 684ms/step - loss: 2.7271e-04 - val_loss: 6.9168e-04\n",
            "Epoch 37/40\n",
            "111/111 [==============================] - 76s 684ms/step - loss: 2.7726e-04 - val_loss: 9.9164e-04\n",
            "Epoch 38/40\n",
            "111/111 [==============================] - 76s 684ms/step - loss: 2.8621e-04 - val_loss: 7.4581e-04\n",
            "Epoch 39/40\n",
            "111/111 [==============================] - 76s 685ms/step - loss: 2.8710e-04 - val_loss: 8.6970e-04\n",
            "Epoch 40/40\n",
            "111/111 [==============================] - 76s 686ms/step - loss: 2.6994e-04 - val_loss: 6.9597e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "nn.save_weights('./checkpoints/my_checkpoint')\n",
        "!zip -r checkpoint.zip checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-nVZXLr8nFr",
        "outputId": "cc3e3759-5d8a-492a-ade4-92aaa0c03679"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: checkpoints/ (stored 0%)\n",
            "  adding: checkpoints/my_checkpoint.data-00000-of-00001 (deflated 8%)\n",
            "  adding: checkpoints/checkpoint (deflated 49%)\n",
            "  adding: checkpoints/my_checkpoint.index (deflated 81%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(nn(test_images[9:10])[0,:,:,1,1].numpy() * 255)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "IQOSFtElcTKN",
        "outputId": "09a05116-78d6-46d4-ce18-04fb96f225bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAADwCAAAAABURuK3AAABJElEQVR4nO3coU7EQBQF0Blads2SYCB4LGYdih/guzD8Dwa5BoNDETAkoCCsaNhCFlr8TrNpMZOGc9wbdXPzxKgXAgAAAAAAAAAAAAAAAAAAAAAAQF8xfSknRf3dZsgySkmBcXoyP75dvGrwj4qjy7u3h4tZ7hyjNZ3fVOv68XQnd5CR2OwpxmJSxPLgbJIlzvgki9Ysn9chxH0F9pMU+PO+ePn6rJ6aHGlGqNyY2+bjeu98dX+1yhJnfDr+gbuzw3pZ+cb0kxYYQgxBfQAAAAAAAAAAAAAAAAAAAAAAAADbxK5rbmwRO0a37wboOtZrCwdID3GHYAcHSDawVd4g7m0DAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMB/8Aty+y0EqNJ04wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=320x240 at 0x7F81460A3610>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(test_labels[9:10][0,:,:,1,1].numpy() * 255)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "VEn_TjQMbICK",
        "outputId": "5194fa62-7e97-4c77-ca76-884f894f7763"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAADwCAAAAABURuK3AAAAeUlEQVR4nO3QsQ0AIAwDwcD+O8MApEsRRdyVrqyPAAAAAAAAAAAAAAAAAAAAAAAAgBnWs5x8Jre7D0wnYJGARQICAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwswuy4QEKDYGjXQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=320x240 at 0x7F8147223850>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf outputs\n",
        "!mkdir outputs\n",
        "for i in range(len(images)):\n",
        "    for part in (0,1,2,3):\n",
        "        y = nn(images[i:i+1])[0,:,:,part,1].numpy() * 255\n",
        "        cv2.imwrite(f'outputs/{i}_{part}.jpg', y)\n",
        "!zip -r outputs.zip outputs"
      ],
      "metadata": {
        "id": "D00iM_18W6NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_A6ENwh9VjMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}